{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple Retrieval QA system using LLM Vinuca 7B locally in Spanish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Rodrigo Gonzalez\n",
    "\n",
    "## Project Description\n",
    "\n",
    "The objective of this notebook is to create a simple example of a Retrieval Question-Answering (QA) system in Spanish using the LLM Vinuca 7B language model (LLM) locally. The system aims to provide accurate and efficient responses to user queries by leveraging the powerful capabilities of LLM Vinuca 7B.\n",
    "\n",
    "## The key components of this project include:\n",
    "\n",
    "1. LLM Vinuca 7B running locally is lauched by using the Text Generation Web UI framework. The model is executed using an NVIDIA GeForce RTX 3060 12GB.\n",
    "\n",
    "2. I want to test how well Vicuna 7B performs working with questions in Spanish. \n",
    "\n",
    "3. The chosen database is one that I assume that the model doesn't have any previous knowledge of. I chose to use the information provided in Wikipedia about Babasónicos, an Argentine rock band that the LLM may be unfamiliar with.\n",
    "\n",
    "4. Chroma DB is used as the vector store DB.\n",
    "\n",
    "4. SBERT is used to create embeddings that will represent each text chunk in the vector DB.\n",
    "\n",
    "5. No \"automagical\" Langchain functions. Langchain provides great classes to perform complex NLP tasks easily. However, the drawback of this approach is that a lot of things are happening \"under the hood,\" and developers often lose some control."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../../requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import langchain\n",
    "import yaml\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the path to the project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.getenv('HOME'), 'JurisGPT')\n",
    "\n",
    "# Change to the project folder\n",
    "os.chdir(PROJECT_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding local libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PROJECT_PATH + '/code/python/libraries')\n",
    "\n",
    "import text_generator_api as tg\n",
    "import jurisgpt_functions as jur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "langchain_api_key = config_data['langchain']['api_key']\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding function\n",
    "\n",
    "The \"all-MiniLM-L6-v2\" is a sentence-transformers model that maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. More information at https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fnc = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Data was previously download from the Babasonicos Wikipedia using the `jus.get_webpage()` funcion and save it in the `babasonicos.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"./data/babasonicos.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Período de actividad\n",
      "\n",
      "1991–presente\n",
      "\n",
      "Discográfica(s)\n",
      "\n",
      "Sony Music (1992\n",
      "\n",
      "\n",
      "\n",
      "1999, 2012\n",
      "\n",
      "\n",
      "\n",
      "presente)\n",
      "\n",
      "Bultaco Discos\n",
      "\n",
      "PopArt Discos (2001\n",
      "\n",
      "\n",
      "\n",
      "2005)\n",
      "\n",
      "Universal Music (2005\n",
      "\n",
      "\n",
      "\n",
      "2012)\n",
      "\n",
      "WebSitio web\n",
      "\n",
      "Babasonicos.com\n",
      "\n",
      "Miembros Adrián «Dárgelos» Rodríguez, Diego «Uma» Rodríguez, Diego «Uma-T» Tuñón, Diego «Pan\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the  document in chunks \n",
    "\n",
    "The NLTK splitter is a \"content-aware\" chunking method. It provides a sentence tokenizer that can split the text into sentences, helping to create more meaningful chunks. More information about different splitting strategies at https://www.pinecone.io/learn/chunking-strategies/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vector database with Chroma\n",
    "\n",
    "Chroma is a AI-native open-source vector database focused on developer productivity and happiness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs_split, embedding_fnc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Next, I will assess the knowledge of the LLM regarding Babasónicos. Therefore, I will pose a question without providing any specific context or prior information. The prompt will be in Spanish, as it is the appropriate language for this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Una conversación entre un usuario curioso y un asistente de inteligencia artificial. \\\n",
    "    El asistente brinda respuestas útiles, detalladas y educadas a las preguntas del usuario.\"\n",
    "question = '¿Conoces a la banda de rock Babasónicos?'\n",
    "prompt = jur.simple_prompt(instruction, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking a question to the LLM...\n",
      "Human question:  ¿Conoces a la banda de rock Babasónicos?\n",
      "LLM response:   Sí, conozco a Babasónicos. Son una banda argentina de rock formada en 1985 en Buenos Aires. Han publicado varios álbumes y han sido muy populares en su país natal y en América Latina. Su música se caracteriza por tener influencias tanto del rock clásico como del pop y el funk.\n",
      "LLM response time: 17.619 seconds.\n"
     ]
    }
   ],
   "source": [
    "print (\"Asking a question to the LLM...\")\n",
    "\n",
    "start_time = jur.tic()\n",
    "response = tg.query_llm(prompt)\n",
    "end_time = jur.toc()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print (\"Human question: \", question)\n",
    "print (\"LLM response: \", response)\n",
    "print(f\"LLM response time: {elapsed_time:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that the LLM knows something about Babasónicos. Nevertheless, it hallucinates a little bit since Babasónicos was formed in 1991, not 1985."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the context\n",
    "\n",
    "Now, let's utilize the information I have already retrieved from Wikipedia to enhance our response and assist the LLM in providing a more accurate answer.\n",
    "\n",
    "I'll utilize the vector database to search for relevant chunks that bear some similarity to the question.\n",
    "\n",
    "I'll ask a new particular question: What year was Babasónicos formed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '¿En qué año se fundó la banda Babasónicos?'\n",
    "\n",
    "docs_query = db.similarity_search(question, k=3)\n",
    "context = '\\n'.join([doc.page_content for doc in docs_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el 2002, Babasónicos se embarca en una gira por EE. UU.\n",
      "\n",
      "y México.\n",
      "\n",
      "A continuación, editaron varios simples con nuevos remixes de los cortes de difusión.\n",
      "\n",
      "El álbum también fue nominado como «Mejor Álbum de Rock» para los Grammy Latinos.\n",
      "\n",
      "Etapa Post-Jessico (2003-2007) Babasónicos en 2006.\n",
      "\n",
      "En el 2003 cerraron la quinta jornada del Quilmes Rock, un festival organizado en la cancha auxiliar de River que convocó a las bandas más importantes del momento.\n",
      "\n",
      "Además, participaron del Cosquín Rock y d\n",
      "----------\n",
      "Babasónicos aparece en 3 temas: «La muerte es mujer», «Arenas Movedizas» y «DJ Beverly Hills».\n",
      "\n",
      "Ese año también editaron su primer disco de lados B, Vórtice Marxista (compuesto por lados B de sus primeros tres discos), el cual se vendía en sus recitales.\n",
      "\n",
      "Ese mismo año, telonearon a la banda irlandesa U2 en sus espectáculos en River, como paso previo al lanzamiento de Miami, lanzado en 1999 y que fue el último disco en el que participa DJ Peggyn quién se desvincula del proyecto por diferencias a\n",
      "----------\n",
      "Período de actividad\n",
      "\n",
      "1991–presente\n",
      "\n",
      "Discográfica(s)\n",
      "\n",
      "Sony Music (1992\n",
      "\n",
      "\n",
      "\n",
      "1999, 2012\n",
      "\n",
      "\n",
      "\n",
      "presente)\n",
      "\n",
      "Bultaco Discos\n",
      "\n",
      "PopArt Discos (2001\n",
      "\n",
      "\n",
      "\n",
      "2005)\n",
      "\n",
      "Universal Music (2005\n",
      "\n",
      "\n",
      "\n",
      "2012)\n",
      "\n",
      "WebSitio web\n",
      "\n",
      "Babasonicos.com\n",
      "\n",
      "Miembros Adrián «Dárgelos» Rodríguez, Diego «Uma» Rodríguez, Diego «Uma-T» Tuñón, Diego «Panza» Castellano, Mariano «Roger» Domínguez\n",
      "\n",
      "Exmiembros Walter «DJ Peggyn» Kebleris, Gabriel «Gabo» Manelli (1969-2008†)\n",
      "\n",
      "Babasónicos es una banda argentina de rock alternativo, formada en el año 1991.\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_query:\n",
    "    print(doc.page_content[:500])\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chroma DB has retrieved 3 chunks containing dates, and the last chunk indicates the year when Babasónicos was formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM query with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking a question to the LLM...\n",
      "Human question:  ¿En qué año se fundó la banda Babasónicos?\n",
      "LLM response:   En 1991.\n",
      "LLM response time: 2.795 seconds.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Usa el siguiente contexto para responder la pregunta en español. Si no sabes la respuesta, solo di que no sabes la respuesta, no trataré de inventar una respuesta.\"\n",
    "prompt = jur.format_prompt(instruction, question, context)\n",
    "\n",
    "print (\"Asking a question to the LLM...\")\n",
    "\n",
    "start_time = jur.tic()\n",
    "response = tg.query_llm(prompt)\n",
    "end_time = jur.toc()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print (\"Human question: \", question)\n",
    "print (\"LLM response: \", response)\n",
    "print(f\"LLM response time: {elapsed_time:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook, I have demonstrated the process of creating a basic Spanish Retrieval QA system using the LLM Vicuña 7B, Chroma DB, SBERT for embeddings, and without relying on any \"automagical\" Langchains features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
